{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ac3445-780f-456f-89fe-68598f90d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sqlalchemy import create_engine\n",
    "import streamlit as st\n",
    "from fastapi import FastAPI\n",
    "import docker\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96038490-2154-463e-9974-477530b38fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/77c639e548884420900845f5f83a5b10', creation_time=1734605230090, experiment_id='2', last_update_time=1734605230090, lifecycle_stage='active', name='Heart Disease Prediction', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure MLflow for DagsHub\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"HarshithReddy-Audipudi\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"Qwerty@123\"\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow\")\n",
    "mlflow.set_experiment(\"Heart Disease Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6b96e0c-62b8-4b6d-a7c9-fb7dbef2ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"heart.csv\"\n",
    "df = pd.read_csv(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f686af-6c28-4dd6-ac5a-d2ac193be642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize and Create Database\n",
    "engine = create_engine('sqlite:///heart_disease.db')\n",
    "# Split data into normalized tables for 3NF\n",
    "patients = df[['age', 'sex', 'cp']].drop_duplicates()\n",
    "heart_stats = df[['age', 'trestbps', 'chol', 'thalach', 'target']]\n",
    "# Save normalized tables\n",
    "patients.to_sql('patients', engine, if_exists='replace', index=False)\n",
    "heart_stats.to_sql('heart_stats', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1be9752-fd5e-4bb5-bbb4-0e0ec2597d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Data with SQL Join Example\n",
    "query = \"\"\"\n",
    "SELECT p.age, p.sex, p.cp, hs.trestbps, hs.chol, hs.thalach, hs.target \n",
    "FROM patients p\n",
    "JOIN heart_stats hs ON p.age = hs.age\n",
    "\"\"\"\n",
    "df = pd.read_sql(query, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c792733d-b61d-4639-8731-eb26d228134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Exploration\n",
    "def explore_data(df):\n",
    "    print(df.info())\n",
    "    print(df.describe())\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\")\n",
    "    plt.title(\"Correlation Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Distributions\n",
    "    df.hist(bins=20, figsize=(15, 10))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Capped values\n",
    "    capped_features = [col for col in df.columns if df[col].max() > df[col].quantile(0.99)]\n",
    "    print(\"Potential capped features:\", capped_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3540a91a-2768-456e-ab67-64dbb3ada397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended cleanup tasks:\n",
      "1. Handle missing values.\n",
      "2. Investigate and address capped features.\n",
      "3. Standardize numerical features.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1402 entries, 0 to 1401\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   age       1402 non-null   int64\n",
      " 1   sex       1402 non-null   int64\n",
      " 2   cp        1402 non-null   int64\n",
      " 3   trestbps  1402 non-null   int64\n",
      " 4   chol      1402 non-null   int64\n",
      " 5   thalach   1402 non-null   int64\n",
      " 6   target    1402 non-null   int64\n",
      "dtypes: int64(7)\n",
      "memory usage: 76.8 KB\n",
      "None\n",
      "               age          sex           cp     trestbps         chol  \\\n",
      "count  1402.000000  1402.000000  1402.000000  1402.000000  1402.000000   \n",
      "mean     54.611983     0.586305     1.186876   131.679743   247.375178   \n",
      "std       8.132684     0.492671     1.036465    17.767095    51.294664   \n",
      "min      29.000000     0.000000     0.000000    94.000000   126.000000   \n",
      "25%      49.000000     0.000000     0.000000   120.000000   212.000000   \n",
      "50%      56.000000     1.000000     1.000000   130.000000   241.000000   \n",
      "75%      60.000000     1.000000     2.000000   140.000000   277.000000   \n",
      "max      77.000000     1.000000     3.000000   200.000000   564.000000   \n",
      "\n",
      "           thalach       target  \n",
      "count  1402.000000  1402.000000  \n",
      "mean    149.248217     0.534950  \n",
      "std      22.480503     0.498955  \n",
      "min      71.000000     0.000000  \n",
      "25%     133.000000     0.000000  \n",
      "50%     153.000000     1.000000  \n",
      "75%     165.000000     1.000000  \n",
      "max     202.000000     1.000000  \n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "thalach     0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/1shgwhjj6895c05ncfv_9c2r0000gn/T/ipykernel_1355/269934932.py:11: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential capped features: ['age', 'trestbps', 'chol', 'thalach']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/1shgwhjj6895c05ncfv_9c2r0000gn/T/ipykernel_1355/269934932.py:16: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Cleanup tasks\n",
    "print(\"Recommended cleanup tasks:\")\n",
    "print(\"1. Handle missing values.\")\n",
    "print(\"2. Investigate and address capped features.\")\n",
    "print(\"3. Standardize numerical features.\")\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb4bf40-43d3-4b3f-98c9-15cb6dfa5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "df['target'] = df['target'].astype('int')  # Ensure target column is integer\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "728d327d-76b6-46a5-9851-13bdf5e00a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Feature Engineering to Both Train and Test Data\n",
    "X_train['bmi_age_comb'] = X_train['age'] * X_train['trestbps']\n",
    "X_test['bmi_age_comb'] = X_test['age'] * X_test['trestbps']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a916e6ce-cc2e-4a0b-8a6d-d73ba0e0fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure active MLflow runs are closed\n",
    "def close_active_run():\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5f7c1dd-f86b-40b0-be5c-1e5501b4eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(classifier, experiment_name, cv_folds=3):\n",
    "    close_active_run()  # Ensure no active runs before starting\n",
    "    with mlflow.start_run(run_name=experiment_name):\n",
    "        numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features)\n",
    "            ])\n",
    "\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                    ('classifier', classifier)])\n",
    "\n",
    "        # Cross-validation\n",
    "        skf = StratifiedKFold(n_splits=cv_folds)\n",
    "        cv_results = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='f1')\n",
    "        print(f\"CV Results (mean/std): {cv_results.mean()} / {cv_results.std()}\")\n",
    "\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Debugging log statements\n",
    "        print(f\"Logging metrics for experiment: {experiment_name}\")\n",
    "        print(f\"F1-score: {f1}, TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "\n",
    "        # Log Parameters\n",
    "        if hasattr(classifier, 'get_params'):\n",
    "            params = classifier.get_params()\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "\n",
    "        mlflow.log_metric(\"F1-score\", f1)\n",
    "        mlflow.log_metric(\"True Positives\", tp)\n",
    "        mlflow.log_metric(\"True Negatives\", tn)\n",
    "        mlflow.log_metric(\"False Positives\", fp)\n",
    "        mlflow.log_metric(\"False Negatives\", fn)\n",
    "\n",
    "        mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "    close_active_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3cd564e-30ca-4f69-9448-1059a94182ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results (mean/std): 0.7284314361287723 / 0.01970018548163284\n",
      "Logging metrics for experiment: Logistic Regression Experiment\n",
      "F1-score: 0.7094594594594594, TP: 105, TN: 90, FP: 41, FN: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:49:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Logistic Regression Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/cd1bc1ee06824bd18aef53594ce63b1f\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #1: Logistic Regression with Preprocessing\n",
    "run_pipeline(LogisticRegression(), \"Logistic Regression Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f28fd1f-1ea3-4dfc-9981-6cf1e60329d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results (mean/std): 0.7262400050047231 / 0.01905962474061427\n",
      "Logging metrics for experiment: Ridge Classifier Experiment\n",
      "F1-score: 0.7070707070707071, TP: 105, TN: 89, FP: 42, FN: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:51:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Ridge Classifier Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/8e1c31b4380f44d7ae2ae91cae23b7e8\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n",
      "CV Results (mean/std): 0.963323216743785 / 0.0025011108755076516\n",
      "Logging metrics for experiment: Random Forest Experiment\n",
      "F1-score: 0.9966777408637874, TP: 150, TN: 130, FP: 1, FN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:51:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Random Forest Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/abfa3f25154b43e1bace9ebe4e95abbc\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n",
      "CV Results (mean/std): 0.9741671319560766 / 0.0066630483258624815\n",
      "Logging metrics for experiment: XGBoost Experiment\n",
      "F1-score: 0.9966555183946488, TP: 149, TN: 131, FP: 0, FN: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:51:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run XGBoost Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/8ecf6ff5b9284848a3df0dfe0d85ff75\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #2: Multiple Classifiers\n",
    "for clf, name in [(RidgeClassifier(), \"Ridge Classifier Experiment\"),\n",
    "                  (RandomForestClassifier(), \"Random Forest Experiment\"),\n",
    "                  (XGBClassifier(), \"XGBoost Experiment\")]:\n",
    "    run_pipeline(clf, name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00322875-19ff-41ce-854f-7bd2fc6b34ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results (mean/std): 0.7284314361287723 / 0.01970018548163284\n",
      "Logging metrics for experiment: Feature Engineering Experiment\n",
      "F1-score: 0.7094594594594594, TP: 105, TN: 90, FP: 41, FN: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:52:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Feature Engineering Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/889c2a85eff8460fbe63acd04e7b489a\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #3: Feature Engineering\n",
    "run_pipeline(LogisticRegression(), \"Feature Engineering Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d340646d-781a-43dc-993a-79af0eeae1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results (mean/std): 0.7284314361287723 / 0.01970018548163284\n",
      "Logging metrics for experiment: Feature Selection Experiment\n",
      "F1-score: 0.7094594594594594, TP: 105, TN: 90, FP: 41, FN: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:52:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Feature Selection Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/0c5fb70969ab4136b802289c587eb1ab\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #4: Feature Selection\n",
    "correlation_threshold = 0.8\n",
    "corr_matrix = X_train.corr()\n",
    "high_corr_features = [column for column in corr_matrix.columns if any(corr_matrix[column] > correlation_threshold)]\n",
    "X_train_selected = X_train.drop(high_corr_features, axis=1)\n",
    "X_test_selected = X_test.drop(high_corr_features, axis=1)\n",
    "run_pipeline(LogisticRegression(), \"Feature Selection Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "463c5895-e1f5-4cf9-8ca5-7c9d0f3654bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/1shgwhjj6895c05ncfv_9c2r0000gn/T/ipykernel_1192/3990109353.py:9: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results (mean/std): 0.7284314361287723 / 0.01970018548163284\n",
      "Logging metrics for experiment: PCA Experiment\n",
      "F1-score: 0.7094594594594594, TP: 105, TN: 90, FP: 41, FN: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:52:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run PCA Experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/c3077f7a90bf4ef88ada74d3ecac7051\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #5: PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(StandardScaler().fit_transform(X_train))\n",
    "X_test_pca = pca.transform(StandardScaler().fit_transform(X_test))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.show()\n",
    "run_pipeline(LogisticRegression(), \"PCA Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45fce12c-92d1-4fd8-8415-dac2edcb2a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run clean-shad-804 at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/9d362ad7bc85439998c6a62d3b963caa\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n",
      "CV Results (mean/std): 0.7284314361287723 / 0.01970018548163284\n",
      "Logging metrics for experiment: Custom Experiment 1\n",
      "F1-score: 0.7094594594594594, TP: 105, TN: 90, FP: 41, FN: 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:52:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Custom Experiment 1 at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/d10e4715283d45f68d78d12c3d20089c\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #6: Custom Experiment (e.g., Hyperparameter Tuning)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'classifier__C': [0.1, 1, 10]}\n",
    "search = GridSearchCV(Pipeline(steps=[('preprocessor', StandardScaler()),\n",
    "                                       ('classifier', LogisticRegression())]),\n",
    "                      param_grid, cv=3)\n",
    "search.fit(X_train, y_train)\n",
    "mlflow.log_params(search.best_params_)\n",
    "run_pipeline(search.best_estimator_, \"Custom Experiment 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e15376ce-e82c-43eb-8d34-27a59a611ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results (mean/std): 0.9596049431126917 / 0.0011288158409103557\n",
      "Logging metrics for experiment: Custom Experiment 2\n",
      "F1-score: 0.9933774834437086, TP: 150, TN: 129, FP: 2, FN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2024/12/20 01:53:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run Custom Experiment 2 at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2/runs/6c1ad8d551974ed683b71cc937f0d5b6\n",
      "🧪 View experiment at: https://dagshub.com/HarshithReddy-Audipudi/HAUDIPUD-P.mlflow/#/experiments/2\n"
     ]
    }
   ],
   "source": [
    "# Experiment #7: Another Custom Experiment\n",
    "run_pipeline(RandomForestClassifier(n_estimators=200, max_depth=10), \"Custom Experiment 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3cde453f-c24e-42be-9fa1-e519e58b667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Random Forest Experiment with F1-score: 0.9966777408637874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633d5bb0f8794ad1b1c18f130ab6c31e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved as 'best_heart_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Select and Save Best Model\n",
    "def select_and_save_best_model():\n",
    "    experiments = [\n",
    "        \"Logistic Regression Experiment\", \"Ridge Classifier Experiment\", \"Random Forest Experiment\",\n",
    "        \"XGBoost Experiment\", \"Feature Engineering Experiment\", \"Feature Selection Experiment\",\n",
    "        \"PCA Experiment\", \"Custom Experiment 1\", \"Custom Experiment 2\"\n",
    "    ]\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "    best_f1_score = 0\n",
    "    best_model_uri = None\n",
    "    best_experiment = None\n",
    "\n",
    "    # Fetch the F1-scores for all experiments\n",
    "    for exp in experiments:\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[mlflow.get_experiment_by_name(\"Heart Disease Prediction\").experiment_id],\n",
    "            filter_string=f\"tags.mlflow.runName = '{exp}'\"\n",
    "        )\n",
    "\n",
    "        if runs:\n",
    "            metrics = runs[0].data.metrics\n",
    "            f1_score = metrics.get(\"F1-score\", 0)\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_uri = runs[0].info.artifact_uri + \"/model\"\n",
    "                best_experiment = exp\n",
    "\n",
    "    if best_model_uri:\n",
    "        print(f\"Best Model: {best_experiment} with F1-score: {best_f1_score}\")\n",
    "        # Load the best model and save it locally\n",
    "        best_model = mlflow.sklearn.load_model(best_model_uri)\n",
    "        joblib.dump(best_model, 'best_heart_model.pkl')\n",
    "        print(\"Best model saved as 'best_heart_model.pkl'\")\n",
    "\n",
    "select_and_save_best_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c4e5ee1-f87d-432e-bc07-378beea82f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score comparison plot saved as 'f1_score_comparison.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/1shgwhjj6895c05ncfv_9c2r0000gn/T/ipykernel_1192/1617740199.py:33: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Generate F1-score comparison plot\n",
    "def generate_f1_plot():\n",
    "    experiments = [\n",
    "        \"Logistic Regression Experiment\", \"Ridge Classifier Experiment\", \"Random Forest Experiment\",\n",
    "        \"XGBoost Experiment\", \"Feature Engineering Experiment\", \"Feature Selection Experiment\",\n",
    "        \"PCA Experiment\", \"Custom Experiment 1\", \"Custom Experiment 2\"\n",
    "    ]\n",
    "    f1_scores = []\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "    # Fetching run IDs for the given experiment names\n",
    "    for exp in experiments:\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[mlflow.get_experiment_by_name(\"Heart Disease Prediction\").experiment_id],\n",
    "            filter_string=f\"tags.mlflow.runName = '{exp}'\"\n",
    "        )\n",
    "\n",
    "        # If there are runs for the experiment, get the latest one\n",
    "        if runs:\n",
    "            metrics = runs[0].data.metrics\n",
    "            f1_scores.append(metrics.get(\"F1-score\", 0))\n",
    "        else:\n",
    "            f1_scores.append(0)  # Default if no run found\n",
    "\n",
    "    # Plot F1-scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(experiments, f1_scores, color='skyblue')\n",
    "    plt.xlabel(\"F1-score\")\n",
    "    plt.title(\"F1-score Comparison Across Experiments\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"f1_score_comparison.png\")\n",
    "    print(\"F1-score comparison plot saved as 'f1_score_comparison.png'\")\n",
    "    plt.show()\n",
    "\n",
    "generate_f1_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3399599-0953-486c-a269-65ef0fbf0b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching metrics for experiment: Logistic Regression Experiment\n",
      "Metrics for Logistic Regression Experiment: {'F1-score': 0.7094594594594594, 'True Positives': 105.0, 'False Positives': 41.0, 'True Negatives': 90.0, 'False Negatives': 45.0}\n",
      "Fetching metrics for experiment: Ridge Classifier Experiment\n",
      "Metrics for Ridge Classifier Experiment: {'False Negatives': 45.0, 'False Positives': 42.0, 'True Negatives': 89.0, 'True Positives': 105.0, 'F1-score': 0.7070707070707071}\n",
      "Fetching metrics for experiment: Random Forest Experiment\n",
      "Metrics for Random Forest Experiment: {'True Negatives': 130.0, 'True Positives': 150.0, 'F1-score': 0.9966777408637874, 'False Positives': 1.0, 'False Negatives': 0.0}\n",
      "Fetching metrics for experiment: XGBoost Experiment\n",
      "Metrics for XGBoost Experiment: {'True Negatives': 131.0, 'False Positives': 0.0, 'False Negatives': 1.0, 'True Positives': 149.0, 'F1-score': 0.9966555183946488}\n",
      "Fetching metrics for experiment: Feature Engineering Experiment\n",
      "Metrics for Feature Engineering Experiment: {'True Positives': 105.0, 'F1-score': 0.7094594594594594, 'False Negatives': 45.0, 'False Positives': 41.0, 'True Negatives': 90.0}\n",
      "Fetching metrics for experiment: Feature Selection Experiment\n",
      "Metrics for Feature Selection Experiment: {'False Positives': 41.0, 'True Negatives': 90.0, 'True Positives': 105.0, 'False Negatives': 45.0, 'F1-score': 0.7094594594594594}\n",
      "Fetching metrics for experiment: PCA Experiment\n",
      "Metrics for PCA Experiment: {'False Positives': 41.0, 'F1-score': 0.7094594594594594, 'True Positives': 105.0, 'True Negatives': 90.0, 'False Negatives': 45.0}\n",
      "Fetching metrics for experiment: Custom Experiment 1\n",
      "Metrics for Custom Experiment 1: {'F1-score': 0.7094594594594594, 'False Positives': 41.0, 'True Positives': 105.0, 'False Negatives': 45.0, 'True Negatives': 90.0}\n",
      "Fetching metrics for experiment: Custom Experiment 2\n",
      "Metrics for Custom Experiment 2: {'True Positives': 150.0, 'False Negatives': 0.0, 'F1-score': 0.9933774834437086, 'False Positives': 2.0, 'True Negatives': 129.0}\n",
      "F1-score comparison plot saved as 'f1_score_comparison_debug.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/1shgwhjj6895c05ncfv_9c2r0000gn/T/ipykernel_1192/3210092896.py:35: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "def generate_f1_plot():\n",
    "    experiments = [\n",
    "        \"Logistic Regression Experiment\", \"Ridge Classifier Experiment\", \"Random Forest Experiment\",\n",
    "        \"XGBoost Experiment\", \"Feature Engineering Experiment\", \"Feature Selection Experiment\",\n",
    "        \"PCA Experiment\", \"Custom Experiment 1\", \"Custom Experiment 2\"\n",
    "    ]\n",
    "    f1_scores = []\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "    # Fetching run IDs for the given experiment names\n",
    "    for exp in experiments:\n",
    "        print(f\"Fetching metrics for experiment: {exp}\")\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[mlflow.get_experiment_by_name(\"Heart Disease Prediction\").experiment_id],\n",
    "            filter_string=f\"tags.mlflow.runName = '{exp}'\"\n",
    "        )\n",
    "\n",
    "        # If there are runs for the experiment, get the latest one\n",
    "        if runs:\n",
    "            metrics = runs[0].data.metrics\n",
    "            print(f\"Metrics for {exp}: {metrics}\")\n",
    "            f1_scores.append(metrics.get(\"F1-score\", 0))\n",
    "        else:\n",
    "            print(f\"No runs found for {exp}\")\n",
    "            f1_scores.append(0)  # Default if no run found\n",
    "\n",
    "    # Plot F1-scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(experiments, f1_scores, color='skyblue')\n",
    "    plt.xlabel(\"F1-score\")\n",
    "    plt.title(\"F1-score Comparison Across Experiments\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"f1_score_comparison_debug.png\")\n",
    "    print(\"F1-score comparison plot saved as 'f1_score_comparison_debug.png'\")\n",
    "    plt.show()\n",
    "\n",
    "generate_f1_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "84895e10-162e-434d-839e-44ee4faa06ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-19 07:54:42.027 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee3f1c85-8523-4c36-ad33-b29919b84570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "069b20f4-9702-4693-9a7a-bf4502c4e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bea67a-e171-4187-a380-4e4c61943122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
